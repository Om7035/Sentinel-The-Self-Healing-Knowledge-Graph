"""
Test script for Phase 1 & 2: Core Library and Scraper Architecture

This script verifies:
1. Models are properly defined with hash computation
2. Scraper factory works with automatic fallback
3. Local scraper can scrape and convert to markdown
4. Firecrawl scraper works if API key is available
"""

import asyncio
import os
from datetime import datetime

from sentinel_core.models import (
    GraphNode,
    TemporalEdge,
    GraphData,
    ScrapeResult,
)
from sentinel_core.scraper import (
    get_scraper,
    get_available_scrapers,
    print_scraper_status,
)


async def test_models():
    """Test Phase 1: Data Models"""
    print("\n" + "="*60)
    print("PHASE 1: Testing Data Models")
    print("="*60)
    
    # Test GraphNode
    node = GraphNode(
        id="test_company",
        label="Company",
        properties={"name": "Test Corp", "industry": "Tech"}
    )
    print(f"\nâœ… GraphNode created: {node.id}")
    
    # Test TemporalEdge with hash computation
    edge = TemporalEdge(
        source="test_company",
        target="john_doe",
        relation="EMPLOYS",
        properties={"role": "CEO"},
        valid_from=datetime.utcnow()
    )
    hash1 = edge.compute_hash()
    print(f"âœ… TemporalEdge created with hash: {hash1[:16]}...")
    
    # Test hash consistency
    edge2 = TemporalEdge(
        source="test_company",
        target="john_doe",
        relation="EMPLOYS",
        properties={"role": "CEO"},
        valid_from=datetime.utcnow()
    )
    hash2 = edge2.compute_hash()
    assert hash1 == hash2, "Hashes should be identical for same content"
    print("âœ… Hash computation is consistent")
    
    # Test GraphData container
    graph_data = GraphData(nodes=[node], edges=[edge])
    print(f"âœ… GraphData created with {len(graph_data.nodes)} nodes and {len(graph_data.edges)} edges")
    
    # Test ScrapeResult
    content = "# Test Page\n\nThis is test content."
    scrape_result = ScrapeResult(
        url="https://example.com",
        content=content,
        content_hash=ScrapeResult.compute_content_hash(content),
        title="Test Page"
    )
    print(f"âœ… ScrapeResult created with hash: {scrape_result.content_hash[:16]}...")
    
    print("\nâœ… Phase 1 Models: ALL TESTS PASSED")


async def test_scraper_factory():
    """Test Phase 2: Scraper Factory"""
    print("\n" + "="*60)
    print("PHASE 2: Testing Scraper Factory")
    print("="*60)
    
    # Print scraper status
    print_scraper_status()
    
    # Get available scrapers
    status = get_available_scrapers()
    print(f"\nğŸ“Š Scraper Availability:")
    for name, available in status.items():
        status_icon = "âœ…" if available else "âŒ"
        print(f"  {status_icon} {name}: {available}")
    
    # Get scraper (should auto-select best available)
    scraper = get_scraper()
    print(f"\nğŸ¯ Selected Scraper: {scraper.get_name()}")
    print(f"âœ… Scraper factory working correctly")


async def test_local_scraper():
    """Test Phase 2: Local Scraper"""
    print("\n" + "="*60)
    print("PHASE 2: Testing Local Scraper")
    print("="*60)
    
    # Force local scraper
    scraper = get_scraper(prefer_local=True)
    print(f"\nğŸ” Using: {scraper.get_name()}")
    
    # Test scraping a simple page
    test_url = "https://example.com"
    print(f"\nğŸ“¥ Scraping: {test_url}")
    
    try:
        result = await scraper.scrape(test_url)
        
        print(f"âœ… Scrape successful!")
        print(f"  - URL: {result.url}")
        print(f"  - Title: {result.title}")
        print(f"  - Content length: {len(result.content)} characters")
        print(f"  - Content hash: {result.content_hash[:16]}...")
        print(f"  - Scraper type: {result.metadata.get('scraper')}")
        print(f"\nğŸ“„ First 200 characters of content:")
        print(f"  {result.content[:200]}...")
        
        # Verify content hash
        expected_hash = ScrapeResult.compute_content_hash(result.content)
        assert result.content_hash == expected_hash, "Content hash mismatch"
        print(f"\nâœ… Content hash verification passed")
        
    except Exception as e:
        print(f"âŒ Scraping failed: {e}")
        raise


async def test_firecrawl_scraper():
    """Test Phase 2: Firecrawl Scraper (if available)"""
    print("\n" + "="*60)
    print("PHASE 2: Testing Firecrawl Scraper")
    print("="*60)
    
    if not os.getenv('FIRECRAWL_API_KEY'):
        print("\nâš ï¸  FIRECRAWL_API_KEY not set, skipping Firecrawl test")
        print("   Set the API key to test premium scraping")
        return
    
    try:
        from sentinel_core.scraper import FirecrawlScraper
        
        try:
            scraper = FirecrawlScraper()
            print(f"\nğŸ” Using: {scraper.get_name()}")
            
            # Test scraping
            test_url = "https://example.com"
            print(f"\nğŸ“¥ Scraping: {test_url}")
            
            result = await scraper.scrape(test_url)
            
            print(f"âœ… Firecrawl scrape successful!")
            print(f"  - URL: {result.url}")
            print(f"  - Title: {result.title}")
            print(f"  - Content length: {len(result.content)} characters")
            print(f"  - Content hash: {result.content_hash[:16]}...")
            print(f"\nğŸ“„ First 200 characters of content:")
            print(f"  {result.content[:200]}...")
        
        except Exception as e:
            print(f"\nâš ï¸  Firecrawl test failed (this is OK if you don't have a valid API key): {e}")
            print("   The local scraper fallback is working, which is the important part!")
            
    except ImportError:
        print("\nâš ï¸  firecrawl-py not installed, skipping Firecrawl test")
        print("   Install with: pip install firecrawl-py")


async def main():
    """Run all tests"""
    print("\n" + "ğŸš€"*30)
    print("SENTINEL PHASE 1 & 2 VERIFICATION")
    print("ğŸš€"*30)
    
    try:
        # Phase 1: Models
        await test_models()
        
        # Phase 2: Scrapers
        await test_scraper_factory()
        await test_local_scraper()
        await test_firecrawl_scraper()
        
        print("\n" + "="*60)
        print("ğŸ‰ ALL TESTS PASSED!")
        print("="*60)
        print("\nâœ… Phase 1: Core Library - COMPLETE")
        print("âœ… Phase 2: Robust Ingestion - COMPLETE")
        print("\nThe library is working correctly and ready for use!")
        
    except Exception as e:
        print("\n" + "="*60)
        print("âŒ TESTS FAILED")
        print("="*60)
        print(f"\nError: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
